{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit-Underfit Diagnosis, Voting algorithms, Bagging and Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datas=pd.read_csv('mpg.csv')\n",
    "datas = datas[datas.horsepower != '?']\n",
    "#datas.describe(include='all')\n",
    "y=datas.iloc[:,0]\n",
    "x=datas.iloc[:,1:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "      <th>car name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
       "0  18.0          8         307.0        130    3504          12.0          70   \n",
       "1  15.0          8         350.0        165    3693          11.5          70   \n",
       "2  18.0          8         318.0        150    3436          11.0          70   \n",
       "3  16.0          8         304.0        150    3433          12.0          70   \n",
       "4  17.0          8         302.0        140    3449          10.5          70   \n",
       "\n",
       "   origin                   car name  \n",
       "0       1  chevrolet chevelle malibu  \n",
       "1       1          buick skylark 320  \n",
       "2       1         plymouth satellite  \n",
       "3       1              amc rebel sst  \n",
       "4       1                ford torino  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      18.0\n",
       "1      15.0\n",
       "2      18.0\n",
       "3      16.0\n",
       "4      17.0\n",
       "5      15.0\n",
       "6      14.0\n",
       "7      14.0\n",
       "8      14.0\n",
       "9      15.0\n",
       "10     15.0\n",
       "11     14.0\n",
       "12     15.0\n",
       "13     14.0\n",
       "14     24.0\n",
       "15     22.0\n",
       "16     18.0\n",
       "17     21.0\n",
       "18     27.0\n",
       "19     26.0\n",
       "20     25.0\n",
       "21     24.0\n",
       "22     25.0\n",
       "23     26.0\n",
       "24     21.0\n",
       "25     10.0\n",
       "26     10.0\n",
       "27     11.0\n",
       "28      9.0\n",
       "29     27.0\n",
       "       ... \n",
       "367    28.0\n",
       "368    27.0\n",
       "369    34.0\n",
       "370    31.0\n",
       "371    29.0\n",
       "372    27.0\n",
       "373    24.0\n",
       "375    36.0\n",
       "376    37.0\n",
       "377    31.0\n",
       "378    38.0\n",
       "379    36.0\n",
       "380    36.0\n",
       "381    36.0\n",
       "382    34.0\n",
       "383    38.0\n",
       "384    32.0\n",
       "385    38.0\n",
       "386    25.0\n",
       "387    38.0\n",
       "388    26.0\n",
       "389    22.0\n",
       "390    32.0\n",
       "391    36.0\n",
       "392    27.0\n",
       "393    27.0\n",
       "394    44.0\n",
       "395    32.0\n",
       "396    28.0\n",
       "397    31.0\n",
       "Name: mpg, Length: 392, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>429.0</td>\n",
       "      <td>198</td>\n",
       "      <td>4341</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>454.0</td>\n",
       "      <td>220</td>\n",
       "      <td>4354</td>\n",
       "      <td>9.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>440.0</td>\n",
       "      <td>215</td>\n",
       "      <td>4312</td>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>455.0</td>\n",
       "      <td>225</td>\n",
       "      <td>4425</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>390.0</td>\n",
       "      <td>190</td>\n",
       "      <td>3850</td>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>383.0</td>\n",
       "      <td>170</td>\n",
       "      <td>3563</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>340.0</td>\n",
       "      <td>160</td>\n",
       "      <td>3609</td>\n",
       "      <td>8.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>400.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3761</td>\n",
       "      <td>9.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>455.0</td>\n",
       "      <td>225</td>\n",
       "      <td>3086</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>113.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2372</td>\n",
       "      <td>15.0</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>198.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2833</td>\n",
       "      <td>15.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>199.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2774</td>\n",
       "      <td>15.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>200.0</td>\n",
       "      <td>85</td>\n",
       "      <td>2587</td>\n",
       "      <td>16.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2130</td>\n",
       "      <td>14.5</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>46</td>\n",
       "      <td>1835</td>\n",
       "      <td>20.5</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>110.0</td>\n",
       "      <td>87</td>\n",
       "      <td>2672</td>\n",
       "      <td>17.5</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>107.0</td>\n",
       "      <td>90</td>\n",
       "      <td>2430</td>\n",
       "      <td>14.5</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>104.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2375</td>\n",
       "      <td>17.5</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>121.0</td>\n",
       "      <td>113</td>\n",
       "      <td>2234</td>\n",
       "      <td>12.5</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6</td>\n",
       "      <td>199.0</td>\n",
       "      <td>90</td>\n",
       "      <td>2648</td>\n",
       "      <td>15.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>215</td>\n",
       "      <td>4615</td>\n",
       "      <td>14.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>200</td>\n",
       "      <td>4376</td>\n",
       "      <td>15.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>210</td>\n",
       "      <td>4382</td>\n",
       "      <td>13.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>193</td>\n",
       "      <td>4732</td>\n",
       "      <td>18.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2130</td>\n",
       "      <td>14.5</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>4</td>\n",
       "      <td>112.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2605</td>\n",
       "      <td>19.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>4</td>\n",
       "      <td>112.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2640</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>4</td>\n",
       "      <td>112.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2395</td>\n",
       "      <td>18.0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>4</td>\n",
       "      <td>112.0</td>\n",
       "      <td>85</td>\n",
       "      <td>2575</td>\n",
       "      <td>16.2</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84</td>\n",
       "      <td>2525</td>\n",
       "      <td>16.0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>4</td>\n",
       "      <td>151.0</td>\n",
       "      <td>90</td>\n",
       "      <td>2735</td>\n",
       "      <td>18.0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>92</td>\n",
       "      <td>2865</td>\n",
       "      <td>16.4</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>74</td>\n",
       "      <td>1980</td>\n",
       "      <td>15.3</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>68</td>\n",
       "      <td>2025</td>\n",
       "      <td>18.2</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>68</td>\n",
       "      <td>1970</td>\n",
       "      <td>17.6</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>63</td>\n",
       "      <td>2125</td>\n",
       "      <td>14.7</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>70</td>\n",
       "      <td>2125</td>\n",
       "      <td>17.3</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2160</td>\n",
       "      <td>14.5</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>4</td>\n",
       "      <td>107.0</td>\n",
       "      <td>75</td>\n",
       "      <td>2205</td>\n",
       "      <td>14.5</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>70</td>\n",
       "      <td>2245</td>\n",
       "      <td>16.9</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>67</td>\n",
       "      <td>1965</td>\n",
       "      <td>15.0</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>67</td>\n",
       "      <td>1965</td>\n",
       "      <td>15.7</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>67</td>\n",
       "      <td>1995</td>\n",
       "      <td>16.2</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>6</td>\n",
       "      <td>181.0</td>\n",
       "      <td>110</td>\n",
       "      <td>2945</td>\n",
       "      <td>16.4</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>6</td>\n",
       "      <td>262.0</td>\n",
       "      <td>85</td>\n",
       "      <td>3015</td>\n",
       "      <td>17.0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>4</td>\n",
       "      <td>156.0</td>\n",
       "      <td>92</td>\n",
       "      <td>2585</td>\n",
       "      <td>14.5</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>6</td>\n",
       "      <td>232.0</td>\n",
       "      <td>112</td>\n",
       "      <td>2835</td>\n",
       "      <td>14.7</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>4</td>\n",
       "      <td>144.0</td>\n",
       "      <td>96</td>\n",
       "      <td>2665</td>\n",
       "      <td>13.9</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84</td>\n",
       "      <td>2370</td>\n",
       "      <td>13.0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>4</td>\n",
       "      <td>151.0</td>\n",
       "      <td>90</td>\n",
       "      <td>2950</td>\n",
       "      <td>17.3</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86</td>\n",
       "      <td>2790</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52</td>\n",
       "      <td>2130</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84</td>\n",
       "      <td>2295</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79</td>\n",
       "      <td>2625</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82</td>\n",
       "      <td>2720</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>392 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
       "0            8         307.0        130    3504          12.0          70   \n",
       "1            8         350.0        165    3693          11.5          70   \n",
       "2            8         318.0        150    3436          11.0          70   \n",
       "3            8         304.0        150    3433          12.0          70   \n",
       "4            8         302.0        140    3449          10.5          70   \n",
       "5            8         429.0        198    4341          10.0          70   \n",
       "6            8         454.0        220    4354           9.0          70   \n",
       "7            8         440.0        215    4312           8.5          70   \n",
       "8            8         455.0        225    4425          10.0          70   \n",
       "9            8         390.0        190    3850           8.5          70   \n",
       "10           8         383.0        170    3563          10.0          70   \n",
       "11           8         340.0        160    3609           8.0          70   \n",
       "12           8         400.0        150    3761           9.5          70   \n",
       "13           8         455.0        225    3086          10.0          70   \n",
       "14           4         113.0         95    2372          15.0          70   \n",
       "15           6         198.0         95    2833          15.5          70   \n",
       "16           6         199.0         97    2774          15.5          70   \n",
       "17           6         200.0         85    2587          16.0          70   \n",
       "18           4          97.0         88    2130          14.5          70   \n",
       "19           4          97.0         46    1835          20.5          70   \n",
       "20           4         110.0         87    2672          17.5          70   \n",
       "21           4         107.0         90    2430          14.5          70   \n",
       "22           4         104.0         95    2375          17.5          70   \n",
       "23           4         121.0        113    2234          12.5          70   \n",
       "24           6         199.0         90    2648          15.0          70   \n",
       "25           8         360.0        215    4615          14.0          70   \n",
       "26           8         307.0        200    4376          15.0          70   \n",
       "27           8         318.0        210    4382          13.5          70   \n",
       "28           8         304.0        193    4732          18.5          70   \n",
       "29           4          97.0         88    2130          14.5          71   \n",
       "..         ...           ...        ...     ...           ...         ...   \n",
       "367          4         112.0         88    2605          19.6          82   \n",
       "368          4         112.0         88    2640          18.6          82   \n",
       "369          4         112.0         88    2395          18.0          82   \n",
       "370          4         112.0         85    2575          16.2          82   \n",
       "371          4         135.0         84    2525          16.0          82   \n",
       "372          4         151.0         90    2735          18.0          82   \n",
       "373          4         140.0         92    2865          16.4          82   \n",
       "375          4         105.0         74    1980          15.3          82   \n",
       "376          4          91.0         68    2025          18.2          82   \n",
       "377          4          91.0         68    1970          17.6          82   \n",
       "378          4         105.0         63    2125          14.7          82   \n",
       "379          4          98.0         70    2125          17.3          82   \n",
       "380          4         120.0         88    2160          14.5          82   \n",
       "381          4         107.0         75    2205          14.5          82   \n",
       "382          4         108.0         70    2245          16.9          82   \n",
       "383          4          91.0         67    1965          15.0          82   \n",
       "384          4          91.0         67    1965          15.7          82   \n",
       "385          4          91.0         67    1995          16.2          82   \n",
       "386          6         181.0        110    2945          16.4          82   \n",
       "387          6         262.0         85    3015          17.0          82   \n",
       "388          4         156.0         92    2585          14.5          82   \n",
       "389          6         232.0        112    2835          14.7          82   \n",
       "390          4         144.0         96    2665          13.9          82   \n",
       "391          4         135.0         84    2370          13.0          82   \n",
       "392          4         151.0         90    2950          17.3          82   \n",
       "393          4         140.0         86    2790          15.6          82   \n",
       "394          4          97.0         52    2130          24.6          82   \n",
       "395          4         135.0         84    2295          11.6          82   \n",
       "396          4         120.0         79    2625          18.6          82   \n",
       "397          4         119.0         82    2720          19.4          82   \n",
       "\n",
       "     origin  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "5         1  \n",
       "6         1  \n",
       "7         1  \n",
       "8         1  \n",
       "9         1  \n",
       "10        1  \n",
       "11        1  \n",
       "12        1  \n",
       "13        1  \n",
       "14        3  \n",
       "15        1  \n",
       "16        1  \n",
       "17        1  \n",
       "18        3  \n",
       "19        2  \n",
       "20        2  \n",
       "21        2  \n",
       "22        2  \n",
       "23        2  \n",
       "24        1  \n",
       "25        1  \n",
       "26        1  \n",
       "27        1  \n",
       "28        1  \n",
       "29        3  \n",
       "..      ...  \n",
       "367       1  \n",
       "368       1  \n",
       "369       1  \n",
       "370       1  \n",
       "371       1  \n",
       "372       1  \n",
       "373       1  \n",
       "375       2  \n",
       "376       3  \n",
       "377       3  \n",
       "378       1  \n",
       "379       1  \n",
       "380       3  \n",
       "381       3  \n",
       "382       3  \n",
       "383       3  \n",
       "384       3  \n",
       "385       3  \n",
       "386       1  \n",
       "387       1  \n",
       "388       1  \n",
       "389       1  \n",
       "390       3  \n",
       "391       1  \n",
       "392       1  \n",
       "393       1  \n",
       "394       2  \n",
       "395       1  \n",
       "396       1  \n",
       "397       1  \n",
       "\n",
       "[392 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=8, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=0.13,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=3, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import DecisionTreeRegressor from sklearn.tree\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import sklearn.model_selection as cv\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "(x_train, x_test, y_train, y_test) = cv.train_test_split(x, y, test_size=.20)\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "                           min_samples_leaf=0.13,\n",
    "                           random_state=3)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of dt: 4.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute y_pred\n",
    "y_pred = dt.predict(x_test)\n",
    "\n",
    "# Compute mse_dt\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute rmse_dt\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: 4.05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Compute the array containing the 10-folds CV MSEs\n",
    "MSE_CV_scores = - cross_val_score(dt, x_train, y_train, cv=10, \n",
    "                                  scoring='neg_mean_squared_error', \n",
    "                                  n_jobs=-1) \n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 3.56\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = dt.predict(x_train)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
    "\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: 4.30\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Set SEED for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split the data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor dt\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=SEED)\n",
    "\n",
    "\n",
    "# Compute the array containing the 10-folds CV MSEs\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n",
    "                                  scoring='neg_mean_squared_error', \n",
    "                                  n_jobs=-1) \n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 4.19\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = dt.predict(X_train)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
    "\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upon reducing the model complexity data suffers from high bias because RMSE_CV ≈ RMSE_train and both scores are greater than baseline_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set SEED for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split the data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor dt\n",
    "dt = DecisionTreeRegressor(max_depth=  6, min_samples_leaf= 0.19 , random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhongyizhang/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# Import function to compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "# Import function to split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "# Import the VotingClassifier meta-model\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "# Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhongyizhang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/zhongyizhang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('breast_data.csv')\n",
    "data.diagnosis[data.diagnosis == 'M'] = 1\n",
    "data.diagnosis[data.diagnosis == 'B'] = 0\n",
    "y=data.iloc[:,1].values\n",
    "y=y.astype('int')\n",
    "cols=['concave points_mean','radius_mean']\n",
    "X=data[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "test_size= 0.3,\n",
    "random_state= SEED)\n",
    "# Instantiate individual classifiers\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "knn = KNN()\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "# Define a list called classifier that containsj\n",
    "# the tuples (classifier_name, classifier)\n",
    "classifiers = [('Logistic Regression', lr),\n",
    "('K Nearest Neighbours', knn),\n",
    "('Classification Tree', dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.865\n",
      "K Nearest Neighbours : 0.883\n",
      "Classification Tree : 0.883\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf in classifiers:\n",
    "#fit clf to the training set\n",
    "    clf.fit(X_train, y_train)\n",
    "# Predict the labels of the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "# Evaluate the accuracy of clf on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name,\n",
    "    accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: 0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhongyizhang/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "vc = VotingClassifier(estimators=classifiers)\n",
    "# Fit 'vc' to the traing set\n",
    "vc.fit(X_train, y_train)\n",
    "# Predict test set labels\n",
    "y_pred = vc.predict(X_test)\n",
    "# Evaluate the test-set accuracy of 'vc'\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y,\n",
    "test_size=0.2,\n",
    "stratify=y,\n",
    "random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bagging Classifier: 0.904\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a classification-tree 'dt'\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16,\n",
    "random_state=SEED)\n",
    "# Instantiate a BaggingClassifier 'bc'\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300,\n",
    "n_jobs=-1)\n",
    "# Fit 'bc' to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "# Evaluate and print test-set accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of Bagging Classifier: {:.3f}'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest and Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests Algorithm\n",
    "The random forests algorithm is very much like the bagging algorithm. Let N be the number of observations and assume for now that the response variable is binary.\n",
    "\n",
    "Take a random sample of size N with replacement from the data (bootstrap sample).\n",
    "\n",
    "Take a random sample without replacement of the predictors.\n",
    "\n",
    "Construct a split by using predictors selected in Step 2.\n",
    "\n",
    "Repeat Steps 2 and 3 for each subsequent split until the tree is as large as desired. Do not prune. Each tree is produced from a random sample of cases and at each split a random sample of predictors.\n",
    "\n",
    "Drop the out-of-bag data down the tree. Store the class assigned to each observation along with each observation's predictor values.\n",
    "\n",
    "Repeat Steps 1-5 a large number of times (e.g., 500).\n",
    "\n",
    "For each observation in the dataset, count the number of trees that it is classified in one category over the number of trees.\n",
    "\n",
    "Assign each observation to a final category by a majority vote over the set of trees. Thus, if 51% of the time over a large number of trees a given observation is classified as a \"1\", that becomes its classification.\n",
    "\n",
    " \n",
    "\n",
    "##### Why Random Forests Work\n",
    "Variance reduction: the trees are more independent because of the combination of bootstrap samples and random draws of predictors.\n",
    "\n",
    "It is apparent that random forests are a form of bagging, and the averaging over trees can substantially reduce instability that might otherwise result. Moreover, by working with a random sample of predictors at each possible split, the fitted values across trees are more independent. Consequently, the gains from averaging over a large number of trees (variance reduction) can be more dramatic.\n",
    "Bias reduction: a very large number of predictors can be considered, and local feature predictors can play a role in tree construction.\n",
    "\n",
    "Random forests are able to work with a very large number of predictors, even more, predictors than there are observations. An obvious gain with random forests is that more information may be brought to reduce bias of fitted values and estimated splits.\n",
    "\n",
    "There are often a few predictors that dominate the decision tree fitting process because on the average they consistently perform just a bit better than their competitors. Consequently, many other predictors, which could be useful for very local features of the data, are rarely selected as splitting variables. With random forests computed for a large enough number of trees, each predictor will have at least several opportunities to be the predictor defining a split. In those opportunities, it will have very few competitors. Much of the time a dominant predictor will not be included. Therefore, local feature predictors will have the opportunity to define a split.\n",
    "\n",
    "Indeed, random forests are among the very best classifiers invented to date (Breiman, 2001a).\n",
    "\n",
    "Random forests include 3 main tuning parameters.\n",
    "\n",
    "Node Size: unlike in decision trees, the number of observations in the terminal nodes of each tree of the forest can be very small. The goal is to grow trees with as little bias as possible.\n",
    "\n",
    "Number of Trees: in practice, 500 trees is often a good choice.\n",
    "\n",
    "Number of Predictors Sampled: the number of predictors sampled at each split would seem to be a key tuning parameter that should affect how well random forests perform. Sampling 2-5 each time is often adequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas=pd.read_csv('mpg.csv')\n",
    "datas = datas[datas.horsepower != '?']\n",
    "y=datas.iloc[:,0]\n",
    "\n",
    "X=datas.iloc[:,1:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "# Split dataset into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y,\n",
    "test_size=0.3,\n",
    "random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 4.24\n",
      "Train set RMSE of rf: 3.44\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a random forests regressor 'rf' 400 estimators\n",
    "rf = RandomForestRegressor(n_estimators=400,\n",
    "min_samples_leaf=0.12,\n",
    "random_state=SEED)\n",
    "# Fit 'rf' to the training set\n",
    "rf.fit(X_train, y_train)\n",
    "# Predict the test set labels 'y_pred'\n",
    "y_pred = rf.predict(X_test)\n",
    "y_pred_train=rf.predict(X_train)\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "rmse_train = MSE(y_train, y_pred_train)**(1/2)\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n",
    "print('Train set RMSE of rf: {:.2f}'.format(rmse_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a pd.Series of features importances\n",
    "importances_rf = pd.Series(rf.feature_importances_,\n",
    "index = X.columns)\n",
    "# Sort importances_rf\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "# Make a horizontal bar plot\n",
    "sorted_importances_rf.plot(kind='barh', color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=pd.read_csv('liver.csv')\n",
    "dff['is_male'] = dff['gender'].apply(lambda x: 1 if x == 'Male' else 0)\n",
    "dff= dff.drop('gender', axis=1)\n",
    "dff.dropna(inplace=True)\n",
    "dff['is_patient']= dff['is_patient'].apply(lambda x: 1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()\n",
    "y=dff.loc[:,'is_patient']\n",
    "\n",
    "X=dff.iloc[:,1:11]\n",
    "X=X.drop(['is_patient'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y,\n",
    "test_size=0.3,\n",
    "stratify=y,\n",
    "random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "# Instantiate ada\n",
    "ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ada to the training set\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Compute the probabilities of obtaining the positive class\n",
    "y_pred_proba = ada.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "ada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(ada_roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=400,\n",
    "min_samples_leaf=0.12,\n",
    "random_state=SEED)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_proba_rf = rf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.68\n"
     ]
    }
   ],
   "source": [
    "#y_pred_proba_rf = rf.predict_proba(X_test)[:,1]\n",
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "rf_roc_auc = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "\n",
    "# Print roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(rf_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED=1\n",
    "\n",
    "# Instantiate lr\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "\n",
    "# Instantiate svm\n",
    "#svm = SVC()\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100,random_state=1)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n",
    "\n",
    "# Define the list classifiers\n",
    "classifiers = [('Logistic Regression', lr), ('Classification Tree', dt),('Random Forest',rf)]#('Support Vectors', svm),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.690\n",
      "Classification Tree : 0.690\n",
      "Random Forest : 0.695\n"
     ]
    }
   ],
   "source": [
    "# Import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf_name, clf in classifiers:\n",
    "  \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)    \n",
    "  \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "  \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting Classifier: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/utkupamuksuz/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import VotingCLassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate a VotingClassifier vc \n",
    "vc = VotingClassifier(estimators=classifiers, voting='hard')     \n",
    "\n",
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train)   \n",
    "\n",
    "# Evaluate the test set predictions\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Hard Voting Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Voting Classifier: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/utkupamuksuz/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import VotingCLassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate a VotingClassifier vc \n",
    "vc = VotingClassifier(estimators=classifiers, voting='soft')     \n",
    "\n",
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train)   \n",
    "\n",
    "# Evaluate the test set predictions\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Soft Voting Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate gb\n",
    "gb = GradientBoostingRegressor(max_depth=4,\n",
    "                               n_estimators=200,\n",
    "                               random_state=2)\n",
    "\n",
    "# Fit gb to the training set\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred_Gboost = gb.predict(X_test)\n",
    "\n",
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute MSE\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_test = mse_test**(1/2)\n",
    "\n",
    "# Print RMSE\n",
    "print('Test set RMSE of gb: {:.3f}'.format(rmse_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
